{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945f5a62012f7b62",
   "metadata": {},
   "source": [
    "1. Collect traces, fit model, sample Weibull posterior):\n",
    "    * Observation To = traces before the shift\n",
    "    * Fit Weibull for Observation To $Wo_0$\n",
    "    * Observation T1 = traces after the shift\n",
    "    * Fit Weibull for Observation T1 $Wo_1$"
   ]
  },
  {
   "cell_type": "code",
   "id": "d08f328440e5ca72",
   "metadata": {},
   "source": [
    "import pytensor\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from pandas.io.json import to_json\n",
    "from pymc import predictions_to_inference_data\n",
    "from scipy.stats import weibull_min\n",
    "\n",
    "pytensor.config.cxx = ''\n",
    "pytensor.config.floatX = \"float64\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "731c5f12f9b7bfc2",
   "metadata": {},
   "source": [
    "def fit_observation_distributions(obs_t0_path, obs_t1_path, should_sample_posterior: bool = False,\n",
    "                                  idata_file_name: str = '', observations: dict[str, pd.DataFrame] = None):\n",
    "    # obs_t0_path = Path().joinpath('distribution_shift', 'traces', 'scratch_s50_f0.5_shift_s50_f0.5', '.summary.csv')\n",
    "    # obs_t1_path = Path().joinpath('distribution_shift', 'traces', 'scratch_s50_f0.5_shift_s50_f1.0', '.summary.csv')\n",
    "\n",
    "    if not observations:\n",
    "        obs_df = {\n",
    "            't0': pd.read_csv(obs_t0_path),\n",
    "            't1': pd.read_csv(obs_t1_path)\n",
    "        }\n",
    "    else:\n",
    "        obs_df = observations\n",
    "\n",
    "    # Transform to prevent 0 values in log calculation\n",
    "    for key, df in obs_df.items():\n",
    "        if 'collisions' in df.columns:\n",
    "            df['collisions'] = df['collisions'] + 0.01\n",
    "\n",
    "    priors_weibull = {\n",
    "        't0': {'collisions': {}, 'waitingTime': {}},\n",
    "        't1': {'collisions': {}, 'waitingTime': {}}\n",
    "    }\n",
    "\n",
    "    obs_weibull = {\n",
    "        't0': {'collisions': pm.Model(), 'waitingTime': pm.Model()},\n",
    "        't1': {'collisions': pm.Model(), 'waitingTime': pm.Model()}\n",
    "    }\n",
    "\n",
    "    idata_weibull = {\n",
    "        't0': {'collisions': None, 'waitingTime': None},\n",
    "        't1': {'collisions': None, 'waitingTime': None}\n",
    "    }\n",
    "\n",
    "    for time in priors_weibull.keys():\n",
    "        for variable in priors_weibull[time].keys():\n",
    "            print(f'Fitting {variable} at {time}')\n",
    "            # Replace with your observed data\n",
    "            data = obs_df[time][variable]\n",
    "\n",
    "            # Fit Weibull using MLE (shape, loc, scale)\n",
    "            priors_weibull[time][variable]['shape'], _, priors_weibull[time][variable]['scale'] = weibull_min.fit(data,\n",
    "                                                                                                                  floc=0)\n",
    "\n",
    "            print(f\"{time} {variable}: Estimated shape (α): {priors_weibull[time][variable]['shape']:.3f}\")\n",
    "            print(f\"{time} {variable}: Estimated scale (β): {priors_weibull[time][variable]['scale']:.3f}\")\n",
    "\n",
    "    for time in obs_weibull.keys():\n",
    "        for variable in obs_weibull[time].keys():\n",
    "            print(f'Fitting {variable} at {time}')\n",
    "            with obs_weibull[time][variable]:\n",
    "                alpha = pm.TruncatedNormal(\"alpha\", mu=priors_weibull[time][variable]['shape'],\n",
    "                                           sigma=priors_weibull[time][variable]['shape'] * 0.5, lower=0.01)\n",
    "                beta = pm.TruncatedNormal(\"beta\", mu=priors_weibull[time][variable]['scale'],\n",
    "                                          sigma=priors_weibull[time][variable]['scale'] * 0.5, lower=0.01)\n",
    "                obs = pm.Weibull(variable, alpha=alpha, beta=beta, observed=obs_df[time][variable])\n",
    "\n",
    "                if idata_file_name:\n",
    "                    idata_path = Path().joinpath('bayesian_models', f'obs_{idata_file_name}_{time}_{variable}.netcdf')\n",
    "                else:\n",
    "                    idata_path = Path().joinpath('bayesian_models', f'obs_weibull_{time}_{variable}.netcdf')\n",
    "\n",
    "                if not idata_path.exists():\n",
    "                    idata = pm.sample()\n",
    "                    idata.to_netcdf(idata_path)\n",
    "                else:\n",
    "                    idata = az.from_netcdf(idata_path)\n",
    "                idata_weibull[time][variable] = idata\n",
    "\n",
    "    predictions = {\n",
    "        't0': {'collisions': None, 'waitingTime': None},\n",
    "        't1': {'collisions': None, 'waitingTime': None}\n",
    "    }\n",
    "\n",
    "    if should_sample_posterior:\n",
    "        for time in obs_weibull.keys():\n",
    "            for variable in obs_weibull[time].keys():\n",
    "                with obs_weibull[time][variable]:\n",
    "                    idata = az.InferenceData.from_netcdf(\n",
    "                        str(Path().joinpath('bayesian_models', f'obs_weibull_{time}_{variable}.netcdf')))\n",
    "                    predictions[time][variable] = pm.sample_posterior_predictive(idata, predictions=True,\n",
    "                                                                                 return_inferencedata=False).predictions\n",
    "\n",
    "    return idata_weibull, predictions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d43f96c2e198e542",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_split_dfs(prediction, obs_df, variable):\n",
    "    data = {}\n",
    "    for time_key in ['t0', 't1']:\n",
    "        # Predicted\n",
    "        pred = pd.DataFrame({\n",
    "            variable: prediction[time_key][variable][variable].values.flatten(),\n",
    "        })\n",
    "        pred['time'] = time_key\n",
    "        pred['source'] = 'predicted'\n",
    "        data[f'{time_key}_predicted'] = pred\n",
    "\n",
    "        # Observed\n",
    "        obs = pd.DataFrame({\n",
    "            variable: obs_df[time_key][variable].values\n",
    "        })\n",
    "        obs['time'] = time_key\n",
    "        obs['source'] = 'observed'\n",
    "        data[f'{time_key}_observed'] = obs\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_comparison(df1, df2, variable, title):\n",
    "    combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(\n",
    "        data=combined,\n",
    "        x=variable,\n",
    "        hue='source',\n",
    "        stat='density',\n",
    "        common_norm=False,  # Key fix!\n",
    "        bins=30,\n",
    "        element='step',\n",
    "        fill=True,\n",
    "        alpha=0.4\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_variable_distributions(prediction, obs_df, variable):\n",
    "    data = extract_split_dfs(prediction, obs_df, variable)\n",
    "\n",
    "    # 1. Predicted vs Observed at same time\n",
    "    for time_key in ['t0', 't1']:\n",
    "        plot_comparison(\n",
    "            data[f'{time_key}_predicted'],\n",
    "            data[f'{time_key}_observed'],\n",
    "            variable,\n",
    "            f'{variable.capitalize()} — {time_key.upper()}: Predicted vs Observed'\n",
    "        )\n",
    "\n",
    "    # 2. Observed t0 vs t1\n",
    "    observed_t0 = data['t0_observed'].copy()\n",
    "    observed_t1 = data['t1_observed'].copy()\n",
    "    observed_t0['source'] = 'observed_t0'\n",
    "    observed_t1['source'] = 'observed_t1'\n",
    "    plot_comparison(observed_t0, observed_t1, variable, f'{variable.capitalize()} — Observed t0 vs t1')\n",
    "\n",
    "    # 3. Predicted t0 vs t1\n",
    "    predicted_t0 = data['t0_predicted'].copy()\n",
    "    predicted_t1 = data['t1_predicted'].copy()\n",
    "    predicted_t0['source'] = 'predicted_t0'\n",
    "    predicted_t1['source'] = 'predicted_t1'\n",
    "    plot_comparison(predicted_t0, predicted_t1, variable, f'{variable.capitalize()} — Predicted t0 vs t1')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ca095bf26e163f2",
   "metadata": {},
   "source": [
    "### Causal Discovery"
   ]
  },
  {
   "cell_type": "code",
   "id": "b3d25a8fb255f837",
   "metadata": {},
   "source": [
    "from castle.common.priori_knowledge import PrioriKnowledge\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from castle.algorithms import PC\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration constants\n",
    "DATA_PATH = Path('structural_causal_models', 'data', 'scm_observational_data.csv')\n",
    "SELECTED_COLUMNS = [\n",
    "    'desiredSpeed', 'friction', 'speed',\n",
    "    'waitingTime', 'emergencyBraking', 'collisions'\n",
    "]\n",
    "INDEPENDENT_VARIABLES = ['desiredSpeed', 'friction']\n",
    "OUTCOME_VARIABLES = ['waitingTime', 'collisions']\n",
    "\n",
    "\n",
    "def create_priori_knowledge(dataframe: pd.DataFrame) -> PrioriKnowledge:\n",
    "    \"\"\"Create prior knowledge constraints for causal discovery.\"\"\"\n",
    "    pk = PrioriKnowledge(len(dataframe.columns))\n",
    "    forbidden_edges = []\n",
    "\n",
    "    # Prevent edges into independent variables\n",
    "    for independent_variable in INDEPENDENT_VARIABLES:\n",
    "        forbidden_edges.extend((column_name, independent_variable) for column_name in dataframe.columns if\n",
    "                               column_name != independent_variable)\n",
    "\n",
    "    # Prevent edges between outcome variables\n",
    "    forbidden_edges.extend(itertools.permutations(OUTCOME_VARIABLES, 2))\n",
    "\n",
    "    # Convert to column indices and add to prior knowledge\n",
    "    edge_indices = [\n",
    "        (dataframe.columns.get_loc(source), dataframe.columns.get_loc(target))\n",
    "        for source, target in forbidden_edges\n",
    "    ]\n",
    "    pk.add_forbidden_edges(edge_indices)\n",
    "\n",
    "    return pk\n",
    "\n",
    "\n",
    "def compute_edge_direction_confidence(graph: nx.DiGraph, dataframe: pd.DataFrame) -> dict:\n",
    "    \"\"\"Compute confidence scores for edge directions using regression residuals.\"\"\"\n",
    "    edge_confidence = {}\n",
    "\n",
    "    for edge in graph.edges():\n",
    "        x, y = edge\n",
    "        x_data = dataframe[x].values.reshape(-1, 1)\n",
    "        y_data = dataframe[y].values.reshape(-1, 1)\n",
    "\n",
    "        # Model X = f(Y)\n",
    "        model_x_y = LinearRegression()\n",
    "        model_x_y.fit(y_data, x_data)\n",
    "        residuals_x = x_data - model_x_y.predict(y_data)\n",
    "\n",
    "        # Model Y = g(X)\n",
    "        model_y_x = LinearRegression()\n",
    "        model_y_x.fit(x_data, y_data)\n",
    "        residuals_y = y_data - model_y_x.predict(x_data)\n",
    "\n",
    "        # Compute Spearman correlations\n",
    "        corr_x_resid, _ = spearmanr(x_data.ravel(), residuals_y.ravel())\n",
    "        corr_y_resid, _ = spearmanr(y_data.ravel(), residuals_x.ravel())\n",
    "\n",
    "        confidence = abs(abs(corr_y_resid) - abs(corr_x_resid))\n",
    "        edge_confidence[(x, y) if abs(corr_x_resid) < abs(corr_y_resid) else (y, x)] = confidence\n",
    "\n",
    "    return edge_confidence\n",
    "\n",
    "\n",
    "def adjust_edges(original_graph: nx.DiGraph, edge_confidence: dict) -> list:\n",
    "    \"\"\"Adjust edge directions based on confidence scores and prior knowledge. If confidence is too low (<0.1) originally discovered edge is preserved\"\"\"\n",
    "    adjusted_edges = []\n",
    "\n",
    "    for edge, confidence in edge_confidence.items():\n",
    "        if edge not in original_graph.edges():\n",
    "            if confidence < 0.1 or edge[1] in INDEPENDENT_VARIABLES:\n",
    "                edge = (edge[1], edge[0])\n",
    "        adjusted_edges.append(edge)\n",
    "\n",
    "    return adjusted_edges\n",
    "\n",
    "\n",
    "def create_adjusted_graph(original_graph: nx.DiGraph, adjusted_edges: list) -> nx.DiGraph:\n",
    "    \"\"\"Create a new graph with adjusted edges.\"\"\"\n",
    "    new_graph = original_graph.copy()\n",
    "    new_graph.clear_edges()\n",
    "    new_graph.add_edges_from(adjusted_edges)\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "def visualize_causal_graph(graph: nx.DiGraph, title: str) -> None:\n",
    "    \"\"\"Visualize the causal graph with specified coloring scheme.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "\n",
    "    color_map = [\n",
    "        'green' if node in INDEPENDENT_VARIABLES else\n",
    "        'red' if graph.out_degree(node) == 0 else\n",
    "        'yellow'\n",
    "        for node in graph.nodes\n",
    "    ]\n",
    "\n",
    "    nx.draw(G=graph, node_color=color_map, node_size=1200, arrowsize=30, with_labels=True,\n",
    "            pos=nx.circular_layout(graph))\n",
    "\n",
    "\n",
    "# Begin\n",
    "df = pd.read_csv(DATA_PATH)[SELECTED_COLUMNS]\n",
    "\n",
    "# Causal discovery setup\n",
    "priori_knowledge = create_priori_knowledge(df)\n",
    "pc = PC(variant='stable', priori_knowledge=priori_knowledge)\n",
    "pc.learn(df.values.tolist())\n",
    "\n",
    "# Graph processing\n",
    "causal_graph = nx.DiGraph(pc.causal_matrix)\n",
    "labeled_graph = nx.relabel_nodes(causal_graph, dict(enumerate(df.columns)))\n",
    "\n",
    "# Edge direction adjustment\n",
    "edge_confidence = compute_edge_direction_confidence(labeled_graph, df)\n",
    "adjusted_edges = adjust_edges(labeled_graph, edge_confidence)\n",
    "adjusted_graph = create_adjusted_graph(labeled_graph, adjusted_edges)\n",
    "\n",
    "# Get The Reverse\n",
    "reversed_graph = adjusted_graph.reverse()\n",
    "reversed_graph.remove_nodes_from(INDEPENDENT_VARIABLES)\n",
    "\n",
    "# Visualization\n",
    "visualize_causal_graph(labeled_graph, \"Discovered Causal Graph\")\n",
    "visualize_causal_graph(adjusted_graph, \"Direction Adjusted Causal Graph\")\n",
    "visualize_causal_graph(reversed_graph, \"Reversed Mediator Causal Graph\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "893e4a0ed6647bab",
   "metadata": {},
   "source": [
    "Get Bayesian Modle From Graph"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc941c17ca531097",
   "metadata": {},
   "source": [
    "import pytensor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import networkx as nx\n",
    "\n",
    "pytensor.config.cxx = ''\n",
    "pytensor.config.floatX = \"float64\"\n",
    "\n",
    "\n",
    "def generate_scm_layers(structural_causal_graph: nx.DiGraph) -> [dict, dict]:\n",
    "    assert nx.is_directed_acyclic_graph(structural_causal_graph)\n",
    "    node_layers = {node: 0 for node in structural_causal_graph.nodes}\n",
    "    predecessors = {node: list(structural_causal_graph.predecessors(node)) for node in structural_causal_graph.nodes}\n",
    "    visited = set(node for node, predecessor_nodes in predecessors.items() if len(predecessor_nodes) == 0)\n",
    "\n",
    "    # Find layers for each node\n",
    "    current_layer = max(node_layers.values()) + 1\n",
    "    while len(visited) < len(structural_causal_graph.nodes):\n",
    "        unprocessed = set(structural_causal_graph.nodes) - visited\n",
    "        current_nodes = [node for node in unprocessed if visited.issuperset(predecessors[node])]\n",
    "        for node in current_nodes:\n",
    "            node_layers[node] = current_layer\n",
    "        visited.update(current_nodes)\n",
    "        current_layer += 1\n",
    "    return node_layers, predecessors\n",
    "\n",
    "\n",
    "def generate_scm_effect_model(data: pd.DataFrame, structural_causal_graph: nx.DiGraph,\n",
    "                              effect: str = 'total') -> pm.Model:\n",
    "    \"\"\"Generate SCM model for either total or direct effects.\"\"\"\n",
    "\n",
    "    if effect not in {'total', 'direct'}:\n",
    "        raise ValueError(\"Effect must be either 'total' or 'direct'\")\n",
    "\n",
    "    node_layers, predecessors = generate_scm_layers(structural_causal_graph)\n",
    "    outcome_variables = [node for node in structural_causal_graph.nodes if\n",
    "                         structural_causal_graph.out_degree(node) == 0]\n",
    "    df_standardized = (data - data.mean()) / data.std()\n",
    "    values = {column: df_standardized[column].values for column in df_standardized.columns}\n",
    "    alpha, beta, sigma, mu, obs, independent_data = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    with pm.Model() as structural_causal_model:\n",
    "        for node in sorted(node_layers.keys(), key=node_layers.__getitem__):\n",
    "            node_should_be_observed = (effect == 'total' and predecessors[node]) or (\n",
    "                    effect == 'direct' and node in outcome_variables)\n",
    "            if node_should_be_observed:\n",
    "                alpha[node] = pm.Normal(f\"{node} alpha\", mu=0, sigma=10)\n",
    "\n",
    "                current_predecessor_string = ', '.join(\n",
    "                    f'{index}:{name}' for index, name in enumerate(predecessors[node]))\n",
    "                current_node_beta_name = f\"{node} beta * ({current_predecessor_string})\"\n",
    "\n",
    "                beta[node] = pm.Normal(current_node_beta_name, mu=0, sigma=10, shape=len(predecessors[node]))\n",
    "                sigma[node] = pm.HalfNormal(f\"{node} sigma\", sigma=1)\n",
    "\n",
    "                mu[node] = alpha[node]\n",
    "                for i, predecessor in enumerate(predecessors[node]):\n",
    "                    if effect == 'total':\n",
    "                        predictor = independent_data[predecessor] if node_layers[predecessor] == 0 else obs[predecessor]\n",
    "                    else:\n",
    "                        predictor = independent_data[predecessor]\n",
    "                    mu[node] += beta[node][i] * predictor\n",
    "\n",
    "                obs[node] = pm.Normal(f\"{node} obs\", mu=mu[node], sigma=sigma[node], observed=values[node])\n",
    "            else:\n",
    "                # Make sure independent variables are modeled\n",
    "                independent_data[node] = pm.Data(f\"{node}_data\", values[node])\n",
    "    return structural_causal_model\n",
    "\n",
    "\n",
    "def shorten_graph_nodes(graph: nx.DiGraph) -> nx.DiGraph:\n",
    "    \"\"\"Renames nodes in a graph copy according to specified abbreviation rules.\"\"\"\n",
    "\n",
    "    def abbreviate(name: str) -> str:\n",
    "        \"\"\"Abbreviates variable names using first letter and uppercase letters.\"\"\"\n",
    "        if not name:\n",
    "            return \"\"\n",
    "        abbreviation = [name[0].lower()]\n",
    "        for c in name[1:]:\n",
    "            if c.isupper():\n",
    "                abbreviation.append(c)\n",
    "        return ''.join(abbreviation)\n",
    "\n",
    "    def process_name(name: str) -> str:\n",
    "        \"\"\"Processes a single node name through all transformation rules.\"\"\"\n",
    "        # Remove parentheses content\n",
    "        clean_name = name.split('*')[0].strip()\n",
    "        parts = clean_name.split('_') if '_' in clean_name else clean_name.split()\n",
    "\n",
    "        if not parts:\n",
    "            return name  # Return original if empty after cleaning\n",
    "\n",
    "        # Process variable part\n",
    "        variable = abbreviate(parts[0])\n",
    "        components = [variable]\n",
    "\n",
    "        # Process remaining parts\n",
    "        i = 1\n",
    "        while i < len(parts):\n",
    "            part = parts[i]\n",
    "            if part == \"alpha\":\n",
    "                components.append(\"α\")\n",
    "            elif part == \"beta\":\n",
    "                components.append(\"β*\")\n",
    "            elif part == \"sigma\":\n",
    "                components.append(\"σ\")\n",
    "            elif part in {\"obs\", \"data\"}:\n",
    "                components.append(part)\n",
    "            i += 1\n",
    "\n",
    "        return ' '.join(components)\n",
    "\n",
    "    # Create mapping and return relabeled graph\n",
    "    return nx.relabel_nodes(graph, {n: process_name(n) for n in graph.nodes}, copy=True)\n",
    "\n",
    "\n",
    "def visualize_scm_model(model: pm.Model) -> None:\n",
    "    model_graph = pm.model_to_networkx(model)\n",
    "    model_graph = shorten_graph_nodes(model_graph)\n",
    "\n",
    "    plt.figure(figsize=(20, 20))  # Set viewport size (width, height in inches)\n",
    "\n",
    "    nx.draw(\n",
    "        model_graph,\n",
    "        pos=nx.planar_layout(model_graph, scale=1),  # Increase node spacing\n",
    "        with_labels=True,\n",
    "        node_size=2500,  # Increase node size (default 300)\n",
    "        node_color='skyblue',  # Better visibility\n",
    "        edge_color='gray',  # Better visibility\n",
    "        width=3,  # Increase edge thickness (default 1)\n",
    "        font_size=14,  # Increase label size\n",
    "        font_weight='bold',  # Improve label readability\n",
    "        arrowsize=15  # Increase arrow size for directed edges\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scm_total_effect_model = generate_scm_effect_model(df, adjusted_graph, effect='total')\n",
    "scm_direct_effect_model = generate_scm_effect_model(df, adjusted_graph, effect='direct')\n",
    "scm_reverse_model = generate_scm_effect_model(df, reversed_graph, effect='total')\n",
    "\n",
    "visualize_scm_model(scm_total_effect_model)\n",
    "visualize_scm_model(scm_direct_effect_model)\n",
    "visualize_scm_model(scm_reverse_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce89608a7e0b3fd0",
   "metadata": {},
   "source": [
    "with scm_reverse_model:\n",
    "    idata = pm.sample()\n",
    "    idata.to_netcdf(Path().joinpath('structural_causal_models', 'scm_reverse.netcdf'))\n",
    "\n",
    "az.summary(idata, round_to=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "549517b52d99832",
   "metadata": {},
   "source": [
    "3. Def: function estimate_posterior_intervention (data, intervention_range)\n",
    "    * z-score normalize the Observation To,\n",
    "    * sample a normal from the fitted SCM,\n",
    "    * de-normalize the data,\n",
    "    * fit the Weibull (to guarantee positive values and make all distributions comparable),\n",
    "    * sample the Weibull back to obtain actual outcome values (collision, waiting time)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff8198dc5a01622c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "SCM_SOURCE_DATA = pd.read_csv(Path().joinpath('structural_causal_models', 'data', 'scm_observational_data.csv'))[\n",
    "    ['desiredSpeed', 'friction', 'speed', 'waitingTime', 'emergencyBraking', 'collisions']]\n",
    "\n",
    "OUTCOME_VARIABLES = ['waitingTime', 'collisions']\n",
    "INTERVENTION_VARIABLES = ['desiredSpeed', 'friction']\n",
    "REVERSED_MODEL_IDATA = Path().joinpath('structural_causal_models', 'scm_reverse.netcdf')\n",
    "\n",
    "\n",
    "def sample_posterior(model: pm.Model, idata: az.InferenceData, input_variables: list,\n",
    "                     standardized_observation_data: pd.DataFrame):\n",
    "    with model:\n",
    "        thinned_idata = idata.sel(chain=[0], draw=slice(None, None, 1000))\n",
    "\n",
    "        # Match Observation Length to Original Input Length for Matrix Multiplication\n",
    "        input_length = len(SCM_SOURCE_DATA)\n",
    "        data_dict = {}\n",
    "        observation_length = len(standardized_observation_data)\n",
    "        number_of_repeats = (input_length + observation_length - 1) // observation_length\n",
    "        for data_variable in input_variables:\n",
    "            variable_values = standardized_observation_data[data_variable].values\n",
    "            repeated_values = np.tile(variable_values, number_of_repeats)[:input_length]\n",
    "            data_dict[f'{data_variable}_data'] = repeated_values\n",
    "\n",
    "        pm.set_data(data_dict)\n",
    "\n",
    "        try:\n",
    "            predictions = pm.sample_posterior_predictive(thinned_idata, predictions=True, return_inferencedata=False)\n",
    "        except ValueError as e:\n",
    "            model.debug(verbose=True)\n",
    "            raise e\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predictions_to_dataframe(predictions, samples: int = None) -> pd.DataFrame:\n",
    "    return pd.DataFrame({column.split()[0]: data.flatten()[:samples] for column, data in predictions.items()})\n",
    "\n",
    "\n",
    "def estimate_posterior_intervention(structural_causal_graph, scm_effect, observation_data, time, friction=0,\n",
    "                                    desired_speed=0):\n",
    "    if scm_effect not in ('total', 'direct'):\n",
    "        raise ValueError(\"scm_effect must be 'total' or 'direct'\")\n",
    "    elif friction != 0 and desired_speed != 0:\n",
    "        raise ValueError(\"Specifying both friction and desired_speed is not currently supported\")\n",
    "    elif time not in ['t0', 't1']:\n",
    "        raise ValueError(\"Time must be either 't0' or 't1'\")\n",
    "\n",
    "    file_name = f'scm_{scm_effect}_effect.netcdf'\n",
    "    file_path = Path().joinpath('structural_causal_models', file_name)\n",
    "    idata = az.InferenceData.from_netcdf(str(file_path))\n",
    "\n",
    "    means = SCM_SOURCE_DATA.mean()\n",
    "    stds = SCM_SOURCE_DATA.std()\n",
    "    standardized_observation_data = (observation_data[\n",
    "                                         ['desiredSpeed', 'friction', 'speed', 'waitingTime', 'emergencyBraking',\n",
    "                                          'collisions']] - means) / stds\n",
    "\n",
    "    # Generate the SCM model\n",
    "    model = generate_scm_effect_model(data=SCM_SOURCE_DATA, structural_causal_graph=structural_causal_graph,\n",
    "                                      effect=scm_effect)\n",
    "    if scm_effect == 'direct':\n",
    "        if time == 't1':\n",
    "            reversed_graph = adjusted_graph.reverse()\n",
    "            reversed_graph.remove_nodes_from(INDEPENDENT_VARIABLES)\n",
    "            reversed_model = generate_scm_effect_model(df, reversed_graph, effect='total')\n",
    "            reversed_idata = az.InferenceData.from_netcdf(str(REVERSED_MODEL_IDATA))\n",
    "            mediators = sample_posterior(model=reversed_model, idata=reversed_idata, input_variables=OUTCOME_VARIABLES,\n",
    "                                         standardized_observation_data=standardized_observation_data)\n",
    "            # bring mediators in same form as observation data\n",
    "            mediators = predictions_to_dataframe(mediators, samples=len(standardized_observation_data))\n",
    "            observation_data[mediators.columns] = mediators\n",
    "\n",
    "        variables = [col for col in standardized_observation_data.columns if col not in OUTCOME_VARIABLES]\n",
    "    else:\n",
    "        variables = INTERVENTION_VARIABLES\n",
    "\n",
    "    predictions = sample_posterior(model=model, idata=idata, input_variables=variables,\n",
    "                                   standardized_observation_data=standardized_observation_data)\n",
    "\n",
    "    predictions = predictions_to_dataframe(predictions)\n",
    "    de_standardized_predictions = pd.DataFrame(predictions)\n",
    "    for column in de_standardized_predictions.keys():\n",
    "        de_standardized_predictions[column] = de_standardized_predictions[column].values * stds[column] + means[column]\n",
    "    de_standardized_predictions = de_standardized_predictions.clip(lower=0.1)\n",
    "\n",
    "    idata_weibull = {}\n",
    "    for variable in de_standardized_predictions[['collisions', 'waitingTime']].columns:\n",
    "        with pm.Model() as weibull_model:\n",
    "            alpha = pm.TruncatedNormal(\"alpha\", mu=1.5,\n",
    "                                       sigma=1, lower=0.01)\n",
    "            beta = pm.TruncatedNormal(\"beta\", mu=de_standardized_predictions[variable].mean(),\n",
    "                                      sigma=de_standardized_predictions[variable].std(), lower=0.01)\n",
    "            obs = pm.Weibull(variable, alpha=alpha, beta=beta, observed=de_standardized_predictions[variable])\n",
    "\n",
    "            idata_weibull[variable] = pm.sample()\n",
    "\n",
    "    return de_standardized_predictions, idata_weibull"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f46ba844fef95f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T01:44:07.981814Z",
     "start_time": "2025-05-01T01:44:07.974855Z"
    }
   },
   "source": [
    "### Interface\n",
    "How to run an experiment\n",
    "\n",
    "Preperation: Set Agent, t0 and t1 config as dataclass with 3 attributes: agent (as a pathlib Path), t0 configuration (with values for friction and desired_speed), t1 configuration (with values for friction and desired_speed)\n",
    "\n",
    "1. load observation for t0 and t1 for given agent and configurations from 1.\n",
    "2. fit weibull for t0 and t1 on observation data\n",
    "3. calculate cdf for t0 and t1 on pymc bayesian weibull with parameterizable constraint value\n",
    "4. Get the difference between the two cdf results\n",
    "\n",
    "All of this has to be done once for collisions and once for waitingTime"
   ]
  },
  {
   "cell_type": "code",
   "id": "47e3344f56f3f5a2",
   "metadata": {},
   "source": [
    "from numpy import exp\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Environment:\n",
    "    desired_speed: int\n",
    "    friction: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    agent: str\n",
    "    t0: Environment\n",
    "    t1: Environment\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.agent}_s{self.t1.desired_speed}_f{self.t1.friction}'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Observation:\n",
    "    t0: pd.DataFrame\n",
    "    t1: pd.DataFrame\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    Experiment('scratch_s50_f1', Environment(50, 1), Environment(50, 0.5)),\n",
    "    Experiment('scratch_s50_f1', Environment(50, 1), Environment(80, 1)),\n",
    "    Experiment('scratch_s80_f0.5', Environment(80, 0.5), Environment(50, 0.5)),\n",
    "    Experiment('scratch_s80_f0.5', Environment(80, 0.5), Environment(80, 1)),\n",
    "    Experiment('scratch_s50_f0.5', Environment(50, 0.5), Environment(80, 0.5)),\n",
    "    Experiment('scratch_s50_f0.5', Environment(50, 0.5), Environment(50, 1)),\n",
    "]\n",
    "\n",
    "\n",
    "def generate_prediction_config(experiment: Experiment):\n",
    "    obs_t0_name = f'{experiment.agent}_shift_s{experiment.t0.desired_speed}_f{float(experiment.t0.friction)}'\n",
    "    obs_t0_path = Path().joinpath('distribution_shift', 'traces', obs_t0_name, '.summary.csv')\n",
    "    if not obs_t0_path.exists():\n",
    "        raise ValueError(f'Could not find observations at {obs_t0_path}, use generate_observations.py to generate')\n",
    "\n",
    "    obs_t1_name = f'{experiment.agent}_shift_s{experiment.t1.desired_speed}_f{float(experiment.t1.friction)}'\n",
    "    obs_t1_path = Path().joinpath('distribution_shift', 'traces', obs_t1_name, '.summary.csv')\n",
    "    if not obs_t1_path.exists():\n",
    "        raise ValueError(f'Could not find observations at {obs_t0_path}, use generate_observations.py to generate')\n",
    "\n",
    "    return obs_t0_path, obs_t1_path\n",
    "\n",
    "\n",
    "def calculate_weibull_cdf_upper(idata: az.InferenceData, value: int):\n",
    "    shape = az.extract(idata).alpha.mean()\n",
    "    scale = az.extract(idata).beta.mean()\n",
    "    cdf_value = weibull_min.cdf(value, c=shape, scale=scale)\n",
    "    return max(0.01, min(0.99, 1 - cdf_value))\n",
    "\n",
    "\n",
    "def calculate_cdf_difference(weibull_t0, weibull_t1, value: int) -> float:\n",
    "    if isinstance(weibull_t0, az.InferenceData):\n",
    "        cdf_t0 = calculate_weibull_cdf_upper(weibull_t0, value)\n",
    "        cdf_t1 = calculate_weibull_cdf_upper(weibull_t1, value)\n",
    "    else:\n",
    "        cdf_t0 = calculate_weibull_cdf_upper_count(weibull_t0, value)\n",
    "        cdf_t1 = calculate_weibull_cdf_upper_count(weibull_t1, value)\n",
    "    if abs(cdf_t0 - cdf_t1) <= 0.01:\n",
    "        return 0.01 if cdf_t0 > 0 else -0.01\n",
    "    return cdf_t0 - cdf_t1\n",
    "\n",
    "def calculate_weibull_cdf_upper_count(data: pd.Series, value: int) -> float:\n",
    "    violations_count = (data > value).sum()\n",
    "    return violations_count / len(data)\n",
    "\n",
    "\n",
    "\n",
    "# for experiment in experiments:\n",
    "#     observation_files = generate_prediction_config(experiment)\n",
    "#     weibull_data, _ = fit_observation_distributions(*observation_files)\n",
    "#     print(calculate_cdf_difference(weibull_data['t0']['collisions'], weibull_data['t1']['collisions'], 20))\n",
    "#     print(calculate_cdf_difference(weibull_data['t0']['waitingTime'], weibull_data['t1']['waitingTime'], 47))\n",
    "#\n",
    "#     print(\"Done\")\n",
    "\n",
    "interventions = ['total', 'direct']\n",
    "times = ['t0', 't1']\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    print(experiment)\n",
    "    obs_t0_path, obs_t1_path = generate_prediction_config(experiment)\n",
    "\n",
    "    obs_df = {\n",
    "        't0': pd.read_csv(obs_t0_path)[\n",
    "            ['desiredSpeed', 'friction', 'speed', 'waitingTime', 'emergencyBraking', 'collisions']],\n",
    "        't1': pd.read_csv(obs_t1_path)[\n",
    "            ['desiredSpeed', 'friction', 'speed', 'waitingTime', 'emergencyBraking', 'collisions']]\n",
    "    }\n",
    "\n",
    "    base_file_name = f\"{experiment.agent}_s{experiment.t1.desired_speed}_f{experiment.t1.friction}\"\n",
    "    for time in times:\n",
    "        for kind in interventions:\n",
    "            file_suffix = f\"{kind}_{time}\"\n",
    "            file_name = f\"{base_file_name}_predictions_{file_suffix}\"\n",
    "            csv_path = Path().joinpath('experiments', 'csv', file_name + \".csv\")\n",
    "\n",
    "            if not csv_path.exists():\n",
    "                predictions_df, weibull_data = estimate_posterior_intervention(adjusted_graph, kind, obs_df[time], time)\n",
    "                predictions_df.to_csv(csv_path, index=False)\n",
    "\n",
    "                for variable, idata in weibull_data.items():\n",
    "                    netcdf_path = Path().joinpath('experiments', 'netcdf', f'{file_name}_{variable}' + \".netcdf\")\n",
    "                    az.InferenceData.to_netcdf(idata, netcdf_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b12edfc56d3cf53b",
   "metadata": {},
   "source": [
    "prediction_values, prediction_weibulls, observation_weibulls = {}, {}, {}\n",
    "interventions = ['total', 'direct']\n",
    "times = ['t0', 't1']\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    observation_files = generate_prediction_config(experiment)\n",
    "    observation_weibulls[str(experiment)], _ = fit_observation_distributions(*observation_files,\n",
    "                                                                             idata_file_name=str(experiment))\n",
    "\n",
    "    prediction_values[str(experiment)] = {'t0': {}, 't1': {}}\n",
    "    prediction_weibulls[str(experiment)] = {'t0': {}, 't1': {}}\n",
    "    for time in times:\n",
    "        for kind in interventions:\n",
    "            base_file_name = f\"{str(experiment)}_predictions_{kind}_{time}\"\n",
    "            prediction_values[str(experiment)][time][kind] = pd.read_csv(\n",
    "                Path().joinpath('experiments', 'csv', base_file_name + '.csv'))[['collisions', 'waitingTime']]\n",
    "\n",
    "            prediction_weibulls[str(experiment)][time][kind] = {}\n",
    "            for variable in ['collisions', 'waitingTime']:\n",
    "                prediction_weibulls[str(experiment)][time][kind][variable] = az.InferenceData.from_netcdf(\n",
    "                    str(Path().joinpath('experiments', 'netcdf', base_file_name + f'_{variable}.netcdf')))\n",
    "        current_time_predictions = {\n",
    "            'total': prediction_values[str(experiment)][time]['total'].copy(),\n",
    "            'direct': prediction_values[str(experiment)][time]['direct'].copy(),\n",
    "        }\n",
    "\n",
    "        #Calculate Weibulls for Indirect Effect\n",
    "    #     current_time_predictions['total']['collisions'] = sorted(current_time_predictions['total']['collisions'])\n",
    "    #     current_time_predictions['direct']['collisions'] = sorted(current_time_predictions['direct']['collisions'])\n",
    "    #     current_time_predictions['total']['waitingTime'] = sorted(current_time_predictions['total']['waitingTime'])\n",
    "    #     current_time_predictions['direct']['waitingTime'] = sorted(current_time_predictions['direct']['waitingTime'])\n",
    "    #     current_time_predictions['indirect']['collisions'] = current_time_predictions['total']['collisions'] - \\\n",
    "    #                                                          current_time_predictions['direct']['collisions']\n",
    "    #     current_time_predictions['indirect']['waitingTime'] = current_time_predictions['total']['waitingTime'] - \\\n",
    "    #                                                           current_time_predictions['direct']['waitingTime']\n",
    "    #     prediction_values[str(experiment)][time]['indirect'] = {}\n",
    "    #     prediction_values[str(experiment)][time]['indirect']['collisions'] = current_time_predictions['indirect'][\n",
    "    #         'collisions']\n",
    "    #     prediction_values[str(experiment)][time]['indirect']['waitingTime'] = current_time_predictions['indirect'][\n",
    "    #         'waitingTime']\n",
    "    #\n",
    "    # observations_indirect = {\n",
    "    #     't0': pd.DataFrame(prediction_values[str(experiment)]['t0']['indirect']),\n",
    "    #     't1': pd.DataFrame(prediction_values[str(experiment)]['t1']['indirect'])\n",
    "    # }\n",
    "    # result = fit_observation_distributions(*observation_files, idata_file_name=str(experiment) + '_indirect',\n",
    "    #                                        observations=observations_indirect)\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    print(\"Collisions > 20\")\n",
    "\n",
    "    print(\"Weibull\", calculate_cdf_difference(observation_weibulls[str(experiment)]['t0']['collisions'],\n",
    "                                   observation_weibulls[str(experiment)]['t1']['collisions'], 20))\n",
    "\n",
    "    print(\"Observations\", calculate_cdf_difference(prediction_values[str(experiment)]['t0']['total']['collisions'],\n",
    "                                   prediction_values[str(experiment)]['t1']['total']['collisions'], 20))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_explanatory_power(observation, prediction):\n",
    "    if observation['delta'] * prediction['delta'] > 0:\n",
    "        return max(0, 1 - abs(observation['delta'] - prediction['delta']) / observation['delta'])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_truncated_difference(cdf_t0, cdf_t1):\n",
    "    if abs(cdf_t0 - cdf_t1) <= 0.01:\n",
    "        return 0.01 if cdf_t0 > 0 else -0.01\n",
    "    return cdf_t0 - cdf_t1\n",
    "\n",
    "\n",
    "def get_explanatory_power_for_value(experiments, variable, value, full_data=False):\n",
    "    cdfs_observations = {}\n",
    "    cdfs_predictions_direct = {}\n",
    "    cdfs_predictions_total = {}\n",
    "    explanatory_powers = {}\n",
    "\n",
    "    for experiment in experiments:\n",
    "        # Observations\n",
    "        cdfs_observations[str(experiment)] = {}\n",
    "        cdfs_predictions_direct[str(experiment)] = {}\n",
    "        cdfs_predictions_total[str(experiment)] = {}\n",
    "\n",
    "        cdfs_observations[str(experiment)]['t0'] = calculate_weibull_cdf_upper(\n",
    "            observation_weibulls[str(experiment)]['t0'][variable], value)\n",
    "        cdfs_observations[str(experiment)]['t1'] = calculate_weibull_cdf_upper(\n",
    "            observation_weibulls[str(experiment)]['t1'][variable], value)\n",
    "\n",
    "        cdfs_observations[str(experiment)]['delta'] = get_truncated_difference(cdfs_observations[str(experiment)]['t0'],\n",
    "                                                                               cdfs_observations[str(experiment)]['t1'])\n",
    "        # Weibulls\n",
    "        cdfs_predictions_direct[str(experiment)]['t0'] = calculate_weibull_cdf_upper(\n",
    "            prediction_weibulls[str(experiment)]['t0']['direct'][variable], value)\n",
    "        cdfs_predictions_direct[str(experiment)]['t1'] = calculate_weibull_cdf_upper(\n",
    "            prediction_weibulls[str(experiment)]['t1']['direct'][variable], value)\n",
    "        cdfs_predictions_direct[str(experiment)]['delta'] = get_truncated_difference(\n",
    "            cdfs_predictions_direct[str(experiment)]['t0'], cdfs_predictions_direct[str(experiment)]['t1'])\n",
    "\n",
    "        cdfs_predictions_total[str(experiment)]['t0'] = calculate_weibull_cdf_upper(\n",
    "            prediction_weibulls[str(experiment)]['t0']['total'][variable], value)\n",
    "        cdfs_predictions_total[str(experiment)]['t1'] = calculate_weibull_cdf_upper(\n",
    "            prediction_weibulls[str(experiment)]['t1']['total'][variable], value)\n",
    "        cdfs_predictions_total[str(experiment)]['delta'] = get_truncated_difference(\n",
    "            cdfs_predictions_total[str(experiment)]['t0'], cdfs_predictions_total[str(experiment)]['t1'])\n",
    "\n",
    "        # Direct Values\n",
    "        # cdfs_predictions_direct[str(experiment)]['t0'] = calculate_weibull_cdf_upper_count(\n",
    "        #     prediction_values[str(experiment)]['t0']['direct'][variable], value)\n",
    "        # cdfs_predictions_direct[str(experiment)]['t1'] = calculate_weibull_cdf_upper_count(\n",
    "        #     prediction_values[str(experiment)]['t1']['direct'][variable], value)\n",
    "        # cdfs_predictions_direct[str(experiment)]['delta'] = get_truncated_difference(\n",
    "        #     cdfs_predictions_direct[str(experiment)]['t0'], cdfs_predictions_direct[str(experiment)]['t1'])\n",
    "        #\n",
    "        # cdfs_predictions_total[str(experiment)]['t0'] = calculate_weibull_cdf_upper_count(\n",
    "        #     prediction_values[str(experiment)]['t0']['total'][variable], value)\n",
    "        # cdfs_predictions_total[str(experiment)]['t1'] = calculate_weibull_cdf_upper_count(\n",
    "        #     prediction_values[str(experiment)]['t1']['total'][variable], value)\n",
    "        # cdfs_predictions_total[str(experiment)]['delta'] = get_truncated_difference(\n",
    "        #     cdfs_predictions_total[str(experiment)]['t0'], cdfs_predictions_total[str(experiment)]['t1'])\n",
    "\n",
    "        explanatory_powers[str(experiment)] = {\n",
    "            'direct': calculate_explanatory_power(cdfs_observations[str(experiment)],\n",
    "                                                  cdfs_predictions_direct[str(experiment)]),\n",
    "            'total': calculate_explanatory_power(cdfs_observations[str(experiment)],\n",
    "                                                 cdfs_predictions_total[str(experiment)])\n",
    "        }\n",
    "\n",
    "    # cdfs_observations, cdfs_predictions_direct, explanatory_powers\n",
    "    if full_data:\n",
    "        return explanatory_powers, cdfs_observations, cdfs_predictions_direct, cdfs_predictions_total\n",
    "    else:\n",
    "        return explanatory_powers\n",
    "\n",
    "explanatory_powers_collisions, cdfs_observations_collisions, cdfs_predictions_direct_collisions, cdfs_predictions_total_collisions = get_explanatory_power_for_value(\n",
    "    experiments, 'collisions', 39, full_data=True)\n",
    "explanatory_powers_waitingTime, cdfs_observations_waitingTime, cdfs_predictions_direct_waitingTime, cdfs_predictions_total_waitingTime = get_explanatory_power_for_value(\n",
    "    experiments, 'waitingTime', 42, full_data=True)\n",
    "\n",
    "\n",
    "# print('explanatory_powers_collisions = ', explanatory_powers_collisions)\n",
    "# print('cdfs_observations_collisions = ', cdfs_observations_collisions)\n",
    "# print('cdfs_predictions_direct_collisions = ', cdfs_predictions_direct_collisions)\n",
    "# print('cdfs_predictions_total_collisions = ', cdfs_predictions_total_collisions)\n",
    "\n",
    "rows = []\n",
    "for exp in explanatory_powers_collisions:\n",
    "    row = {\n",
    "        'experiment with c>20 and wT>45': exp,\n",
    "        'delta_cdf_observation_c': cdfs_observations_collisions[exp]['delta'],\n",
    "        'delta_cdf_observation_wT': cdfs_observations_waitingTime[exp]['delta'],\n",
    "        'delta_cdf_prediction_direct_c': cdfs_predictions_direct_collisions[exp]['delta'],\n",
    "        'delta_cdf_prediction_direct_wT': cdfs_predictions_direct_waitingTime[exp]['delta'],\n",
    "        'delta_cdf_prediction_total_c': cdfs_predictions_total_collisions[exp]['delta'],\n",
    "        'delta_cdf_prediction_total_wT': cdfs_predictions_total_waitingTime[exp]['delta'],\n",
    "        'explanatory_power_direct_c': explanatory_powers_collisions[exp]['direct'],\n",
    "        'explanatory_power_direct_wT': explanatory_powers_waitingTime[exp]['direct'],\n",
    "        'explanatory_power_total_c': explanatory_powers_collisions[exp]['total'],\n",
    "        'explanatory_power_total_wT': explanatory_powers_waitingTime[exp]['total']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Create the DataFrame with specified column order and set index\n",
    "columns_order = [\n",
    "    'delta_cdf_observation_c',\n",
    "    'delta_cdf_observation_wT',\n",
    "    'delta_cdf_prediction_direct_c',\n",
    "    'delta_cdf_prediction_direct_wT',\n",
    "    'delta_cdf_prediction_total_c',\n",
    "    'delta_cdf_prediction_total_wT',\n",
    "    'explanatory_power_direct_c',\n",
    "    'explanatory_power_direct_wT',\n",
    "    'explanatory_power_total_c',\n",
    "    'explanatory_power_total_wT'\n",
    "]\n",
    "df = pd.DataFrame(rows).set_index('experiment with c>20 and wT>45')[columns_order].round(4)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ],
   "id": "6af9fae784a89dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "experiment_groups = {\n",
    "    's50_f1': ['scratch_s50_f1_s50_f0.5', 'scratch_s50_f1_s80_f1'],\n",
    "    's80_f0.5': ['scratch_s80_f0.5_s50_f0.5', 'scratch_s80_f0.5_s80_f1'],\n",
    "    's50_f0.5': ['scratch_s50_f0.5_s80_f0.5', 'scratch_s50_f0.5_s50_f1'],\n",
    "}\n",
    "\n",
    "explanatory_powers_per_value = {}\n",
    "for i in tqdm(range(100)):\n",
    "    explanatory_powers_per_value[i] = {\n",
    "        'collisions': get_explanatory_power_for_value(experiments, 'collisions', i),\n",
    "        'waitingTime': get_explanatory_power_for_value(experiments, 'waitingTime', i)\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "\n",
    "for value, categories in explanatory_powers_per_value.items():\n",
    "    collisions = categories['collisions']\n",
    "    waiting_time = categories['waitingTime']\n",
    "\n",
    "    # All experiments from collisions keys\n",
    "    for experiment in collisions:\n",
    "        row = {\n",
    "            'value': value,\n",
    "            'experiment': experiment,\n",
    "            'collisions_direct': collisions[experiment]['direct'],\n",
    "            'collisions_total': collisions[experiment]['total'],\n",
    "            'waitingTime_direct': waiting_time[experiment]['direct'],\n",
    "            'waitingTime_total': waiting_time[experiment]['total']\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "for group_name, group_experiments in experiment_groups.items():\n",
    "    # Filter data for the current group\n",
    "    group_df = df[df['experiment'].isin(group_experiments)]\n",
    "\n",
    "    if group_df.empty:\n",
    "        print(f\"No data available for group: {group_name}\")\n",
    "        continue\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Experiment Group: {group_name}', fontsize=16)\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "    # Define columns to plot\n",
    "    y_columns = ['collisions_direct', 'collisions_total',\n",
    "                 'waitingTime_direct', 'waitingTime_total']\n",
    "\n",
    "    # Generate distinct colors for each experiment\n",
    "    unique_experiments = group_df['experiment'].unique()\n",
    "    colors = plt.cm.tab10(range(len(unique_experiments)))\n",
    "\n",
    "    for ax, col in zip(axes, y_columns):\n",
    "        for idx, exp in enumerate(unique_experiments):\n",
    "            exp_data = group_df[group_df['experiment'] == exp]\n",
    "\n",
    "            # Add jitter to x and y values\n",
    "            jitter_x = exp_data['value'] + np.random.normal(0, 0.005, size=len(exp_data))\n",
    "            jitter_y = exp_data[col] + np.random.normal(0, 0.001, size=len(exp_data))\n",
    "\n",
    "            ax.scatter(jitter_x, jitter_y,\n",
    "                       color=colors[idx], label=exp, alpha=0.7)\n",
    "        ax.set_xlabel('Constraint > X')\n",
    "        ax.set_ylabel(f'explanatory power (Pw)  {\" \".join(col.split(\"_\"))} effect')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Create a unified legend outside the plots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right',\n",
    "               bbox_to_anchor=(1.15, 0.9), title='Experiments')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "print(\"End\")"
   ],
   "id": "4fa12ca3e5f04fe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b5e59be0b63c7514",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
